{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "localize_text_tesseract.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvj4pLq_sjMP"
      },
      "source": [
        "# Tesseract-OCR 설치\n",
        "   - 이미지 내 문자 추출(OCR) 오픈 소스\n",
        "   - OCR(Optical Character Recognition, 광학 문자 인식)\n",
        "      인쇄물 또는 사진 상의 글자와 이미지를 디지털 데이터로 변환해주는 자동인식기술\n",
        "   - HP가 1980년대 처음 개발\n",
        "   - 2006년 9월 구글이 디버깅을 지원하고 오픈소스로 공개      \n",
        "      \n",
        "# pytesseract 설치\n",
        "- Python Tesseract Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBiebKksApj"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!sudo apt-get install tesseract-ocr-kor\n",
        "!pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZZ80-TfurZH"
      },
      "source": [
        "# 이미지 Upload"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0wHGOAJtNI-"
      },
      "source": [
        "!mkdir input\n",
        "!wget https://github.com/jeeenn/pis_study/raw/master/localize_text_tesseract/apple_support.png -P /content/input\n",
        "!wget https://github.com/jeeenn/pis_study/raw/master/localize_text_tesseract/naver_home.png -P /content/input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQRGCFfWtBkT"
      },
      "source": [
        "# USAGE\n",
        "# python localize_text_tesseract.py --image apple_support.png\n",
        "# python localize_text_tesseract.py --image apple_support.png --min-conf 50\n",
        "\n",
        "# import the necessary packages\n",
        "from pytesseract import Output\n",
        "import pytesseract\n",
        "#import argparse\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUEJUsTHwYAD"
      },
      "source": [
        "# construct the argument parser and parse the arguments\n",
        "#ap = argparse.ArgumentParser()\n",
        "#ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "#\thelp=\"path to input image to be OCR'd\")\n",
        "#ap.add_argument(\"-c\", \"--min-conf\", type=int, default=0,\n",
        "#\thelp=\"mininum confidence value to filter weak text detection\")\n",
        "#args = vars(ap.parse_args())\n",
        "\n",
        "image_input = '/content/input/apple_support.png'\n",
        "#image_input = '/content/input/naver_home.png'\n",
        "min_conf = 50"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMVV2VQ4tJio"
      },
      "source": [
        "# load the input image, convert it from BGR to RGB channel ordering,\n",
        "# and use Tesseract to localize each area of text in the input image\n",
        "#image = cv2.imread(args[\"image\"])\n",
        "image = cv2.imread(image_input)\n",
        "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "cv2_imshow(image)\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRAb6VSJ1Gty"
      },
      "source": [
        "results = pytesseract.image_to_data(rgb, output_type=Output.DICT)\n",
        "#results = pytesseract.image_to_data(rgb, lang='kor',output_type=Output.DICT)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6nZ4eTwwlV4"
      },
      "source": [
        "# loop over each of the individual text localizations\n",
        "for i in range(0, len(results[\"text\"])):\n",
        "  # extract the bounding box coordinates of the text region from\n",
        "  # the current result\n",
        "  x = results[\"left\"][i]\n",
        "  y = results[\"top\"][i]\n",
        "  w = results[\"width\"][i]\n",
        "  h = results[\"height\"][i]\n",
        "\n",
        "  # extract the OCR text itself along with the confidence of the\n",
        "  # text localization\n",
        "  text = results[\"text\"][i]\n",
        "  conf = int(results[\"conf\"][i])\n",
        "\n",
        "  # filter out weak confidence text localizations\n",
        "  # if conf > args[\"min_conf\"]:\n",
        "  if conf > min_conf:\n",
        "    # display the confidence and text to our terminal\n",
        "    print(\"Confidence: {}\".format(conf))\n",
        "    print(\"Text: {}\".format(text))\n",
        "    print(\"\")\n",
        "\n",
        "    # strip out non-ASCII text so we can draw the text on the image\n",
        "    # using OpenCV, then draw a bounding box around the text along\n",
        "    # with the text itself\n",
        "    text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
        "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "    cv2.putText(image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                  1.2, (0, 0, 255), 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2axDCtVwrMO"
      },
      "source": [
        "# show the output image\n",
        "#cv2.imshow(\"Image\", image)'\n",
        "cv2_imshow(image)\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}