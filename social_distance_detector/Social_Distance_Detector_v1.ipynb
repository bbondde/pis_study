{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Social Distance Detector_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejqRVrVrZNDv",
        "colab_type": "text"
      },
      "source": [
        "# What is social distancing?\n",
        " ![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2020/05/social_distance_detector_example.png)\n",
        " -  Using computer vision technology based on OpenCV and YOLO-based deep learning, we are able to estimate the social distance of people in video streams. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdVXrr3QFclq",
        "colab_type": "text"
      },
      "source": [
        "OpenCV ( Open Source Computer Vision )은 실시간 컴퓨터 비전을 목적으로 한 프로그래밍 라이브러리이다. 원래는 인텔이 개발하였다. 실시간 이미지 프로세싱에 중점을 둔 라이브러리이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlnsI-K_aB3J",
        "colab_type": "text"
      },
      "source": [
        "## Using OpenCV, computer vision, and deep learning for social distancing\n",
        "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2020/05/social_distance_detector_steps.png)\n",
        "...\n",
        "- We can use OpenCV, computer vision, and deep learning to implement social distancing detectors.\n",
        "\n",
        "- The steps to build a social distancing detector include:\n",
        "\n",
        "1. Apply object detection to detect all people (and only people) in a video stream (see this tutorial on building an OpenCV people counter)\n",
        "\n",
        "2. Compute the pairwise distances between all detected people\n",
        "\n",
        "3. Based on these distances, check to see if any two people are less than N pixels apart\n",
        "\n",
        "Our OpenCV social distancing detector implementation will rely on pixel distances \n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nDr1XOGg4Do",
        "colab_type": "text"
      },
      "source": [
        "## Project structure\n",
        "\n",
        "```\n",
        "OpenCV Social Distancing Detector\n",
        "$ tree --dirsfirst\n",
        ".\n",
        "├── pyimagesearch\n",
        "│   ├── __init__.py\n",
        "│   ├── detection.py\n",
        "│   └── social_distancing_config.py\n",
        "├── yolo-coco\n",
        "│   ├── coco.names\n",
        "│   ├── yolov3.cfg\n",
        "│   └── yolov3.weights\n",
        "├── output.avi\n",
        "├── pedestrians.mp4\n",
        "└── social_distance_detector.py\n",
        "2 directories, 9 files\n",
        "\n",
        "```\n",
        "\n",
        "Our YOLO object detector files including the CNN architecture definition, pre-trained weights, and class names are housed in the yolo-coco/ directory. This YOLO model is compatible with OpenCV’s DNN module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgWJhEp_Sfl-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## * social_distancing_config.py: A Python file holding a number of constants in one convenient place.\n",
        "(from pyimagesearch import social_distancing_config as config)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y2JlCKcScO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# base path to YOLO directory\n",
        "MODEL_PATH = \"yolo-coco\"\n",
        "\n",
        "# initialize minimum probability to filter weak detections along with\n",
        "# the threshold when applying non-maxima suppression\n",
        "MIN_CONF = 0.3\n",
        "NMS_THRESH = 0.3\n",
        "\n",
        "# boolean indicating if NVIDIA CUDA GPU should be used \n",
        "#(requires that OpenCV’s “dnn” module be installed with NVIDIA GPU support).\n",
        "USE_GPU = False\n",
        "\n",
        "# define the minimum safe distance (in pixels) that two people can be\n",
        "# from each other\n",
        "MIN_DISTANCE = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyAdXiB7Sn4X",
        "colab_type": "text"
      },
      "source": [
        "### * detection.py :  YOLO object detection with OpenCV \n",
        "( from pyimagesearch.detection import detect_people)\n",
        "- Our detect_people utility function, which detects people in video streams using the YOLO object detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ2wTqsxkpuX",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "*detect_people*; the function accepts four parameters:\n",
        "\n",
        "- frame: The frame from your video file or directly from your webcam\n",
        "- net: The pre-initialized and pre-trained YOLO object detection model\n",
        "- ln: The YOLO CNN output layer names\n",
        "- personIdx: The YOLO model can detect many types of objects; this index is specifically for the person class, as we won’t be considering other objects\n",
        "\n",
        "We then initialize our results list, which the function ultimately returns.\n",
        "The results consist of (1) the person prediction probability, (2) bounding box coordinates for the detection, and (3) the centroid of the object.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6p8e_XbSmHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "# from .social_distancing_config import NMS_THRESH\n",
        "# from .social_distancing_config import MIN_CONF\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# frame : 탐지 대상 영상\n",
        "# net : YOLO 모델\n",
        "# in : YOLO 모델의 출력 레이어 이름들. ['yolo_82', 'yolo_94', 'yolo_106']\n",
        "# personIdx : 카테고리 person의 index.\n",
        "def detect_people(frame, net, ln, personIdx=0):\n",
        "\t# grab the dimensions of the frame and  initialize the list of\n",
        "\t# results\n",
        "\t(H, W) = frame.shape[:2]\n",
        "\tresults = []\n",
        "\n",
        "\t# construct a blob from the input frame and then perform a forward\n",
        "\t# pass of the YOLO object detector, giving us our bounding boxes\n",
        "\t# and associated probabilities\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
        "\t\tswapRB=True, crop=False)\n",
        "\tnet.setInput(blob)\n",
        "\tlayerOutputs = net.forward(ln)\n",
        "\n",
        "\t# initialize our lists of detected bounding boxes, centroids, and\n",
        "\t# confidences, respectively\n",
        "\tboxes = []\n",
        "\tcentroids = []\n",
        "\tconfidences = []\n",
        "\n",
        "\t# loop over each of the layer outputs\n",
        "\tfor output in layerOutputs:\n",
        "\t\t# loop over each of the detections\n",
        "\t\tfor detection in output:\n",
        "\t\t\t# extract the class ID and confidence (i.e., probability)\n",
        "\t\t\t# of the current object detection\n",
        "\t\t\tscores = detection[5:]\n",
        "\t\t\tclassID = np.argmax(scores)\n",
        "\t\t\tconfidence = scores[classID]\n",
        "\n",
        "\t\t\t# filter detections by (1) ensuring that the object\n",
        "\t\t\t# detected was a person and (2) that the minimum\n",
        "\t\t\t# confidence is met\n",
        "\t\t\tif classID == personIdx and confidence > MIN_CONF:\n",
        "\t\t\t\t# scale the bounding box coordinates back relative to\n",
        "\t\t\t\t# the size of the image, keeping in mind that YOLO\n",
        "\t\t\t\t# actually returns the center (x, y)-coordinates of\n",
        "\t\t\t\t# the bounding box followed by the boxes' width and\n",
        "\t\t\t\t# height\n",
        "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
        "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t\t# use the center (x, y)-coordinates to derive the top\n",
        "\t\t\t\t# and and left corner of the bounding box\n",
        "\t\t\t\tx = int(centerX - (width / 2))\n",
        "\t\t\t\ty = int(centerY - (height / 2))\n",
        "\n",
        "\t\t\t\t# update our list of bounding box coordinates,\n",
        "\t\t\t\t# centroids, and confidences\n",
        "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
        "\t\t\t\tcentroids.append((centerX, centerY))\n",
        "\t\t\t\tconfidences.append(float(confidence))\n",
        "\n",
        "\t# apply non-maxima suppression to suppress weak, overlapping\n",
        "\t# bounding boxes\n",
        "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
        "\n",
        "\t# ensure at least one detection exists\n",
        "\tif len(idxs) > 0:\n",
        "\t\t# loop over the indexes we are keeping\n",
        "\t\tfor i in idxs.flatten():\n",
        "\t\t\t# extract the bounding box coordinates\n",
        "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
        "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "\t\t\t# update our results list to consist of the person\n",
        "\t\t\t# prediction probability, bounding box coordinates,\n",
        "\t\t\t# and the centroid\n",
        "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
        "\t\t\tresults.append(r)\n",
        "\n",
        "\t# return the list of results\n",
        "\treturn results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6yHlNDcTEGv",
        "colab_type": "text"
      },
      "source": [
        "### Social Distance Detector\n",
        "(social_distance_detector.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZobH5kGQVyZ",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title 기본 제목 텍스트\n",
        "# USAGE\n",
        "# python social_distance_detector.py --input pedestrians.mp4\n",
        "# python social_distance_detector.py --input pedestrians.mp4 --output output.avi\n",
        "\n",
        "# import the necessary packages\n",
        "# from pyimagesearch import social_distancing_config as config\n",
        "# from pyimagesearch.detection import detect_people\n",
        "from scipy.spatial import distance as dist\n",
        "import numpy as np\n",
        "# import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# construct the argument parse and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\n",
        "# \thelp=\"path to (optional) input video file\")\n",
        "# ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\n",
        "# \thelp=\"path to (optional) output video file\")\n",
        "# ap.add_argument(\"-d\", \"--display\", type=int, default=1,\n",
        "# \thelp=\"whether or not output frame should be displayed\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "args = {'input':'pedestrians.mp4','output':'output.avi','display':0}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SJGJ39dOxE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "311b1b1d-9567-46e5-d4f1-8b8f2d453af1"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-23 00:18:12--  https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8342 (8.1K) [text/plain]\n",
            "Saving to: ‘yolov3.cfg’\n",
            "\n",
            "\ryolov3.cfg            0%[                    ]       0  --.-KB/s               \ryolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-23 00:18:13 (92.3 MB/s) - ‘yolov3.cfg’ saved [8342/8342]\n",
            "\n",
            "--2020-07-23 00:18:16--  https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 625 [text/plain]\n",
            "Saving to: ‘coco.names’\n",
            "\n",
            "coco.names          100%[===================>]     625  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-23 00:18:16 (41.1 MB/s) - ‘coco.names’ saved [625/625]\n",
            "\n",
            "--2020-07-23 00:18:19--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M   712KB/s    in 5m 2s   \n",
            "\n",
            "2020-07-23 00:23:23 (801 KB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wHAltJBPla9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir yolo-coco\n",
        "!mv yolov3.cfg yolo-coco/\n",
        "!mv coco.names yolo-coco/\n",
        "!mv yolov3.weights yolo-coco/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN5hxpCEBpa_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "e54299e3-b878-4217-b5b5-9cd813018383"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN4bKOK7VH8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/pedestrians.mp4' ./  #(./) 현재위치로\n",
        "#!wget /pedestrians.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfTNxPfnb2l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # load the COCO class labels our YOLO model was trained on\n",
        "# labelsPath = os.path.sep.join([config.MODEL_PATH, \"coco.names\"])\n",
        "# LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
        "labelsPath = os.path.sep.join([MODEL_PATH, \"coco.names\"])\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPiiHj0viu5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(LABELS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8BkQEewdW8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# derive the paths to the YOLO weights and model configuration\n",
        "weightsPath = os.path.sep.join([MODEL_PATH, \"yolov3.weights\"])\n",
        "configPath = os.path.sep.join([MODEL_PATH, \"yolov3.cfg\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWjjytBOkCot",
        "colab_type": "text"
      },
      "source": [
        "### Using OpenCV’s DNN module, we load our YOLO net into memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V8oLrOXdkbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ea2fd8f6-4def-41ac-9803-cadbdaf7f8ea"
      },
      "source": [
        "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "print(\"[INFO] loading YOLO from disk...\")\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading YOLO from disk...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QytcFBlrUMRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4adac2d-75de-4b71-cac6-afe7dc3dcdf9"
      },
      "source": [
        "print(net.getUnconnectedOutLayersNames())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['yolo_82', 'yolo_94', 'yolo_106']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAX4_HI5iORt",
        "colab_type": "text"
      },
      "source": [
        "If you have the USE_GPU option set in the config, then the backend processor is set to be your NVIDIA CUDA-capable GPU. If you don’t have a CUDA-capable GPU, ensure that the configuration option is set to False so that your CPU is the processor used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQOiTQThdpD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check if we are going to use GPU\n",
        "#if USE_GPU == True:\n",
        "\t# set CUDA as the preferable backend and target\n",
        "#print(\"[INFO] setting preferable backend and target to CUDA...\")\n",
        "#net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
        "#net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNnxHlw5e0e8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "f0de5cba-fc49-49e9-f8a6-468e2701db12"
      },
      "source": [
        "# determine only the *output* layer names that we need from YOLO\n",
        "ln = net.getLayerNames()\n",
        "print(\"ln :\", ln)\n",
        "# ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "ln = net.getUnconnectedOutLayersNames()\n",
        "print(net.getUnconnectedOutLayers())\n",
        "print(ln)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ln : ['conv_0', 'bn_0', 'relu_0', 'conv_1', 'bn_1', 'relu_1', 'conv_2', 'bn_2', 'relu_2', 'conv_3', 'bn_3', 'relu_3', 'shortcut_4', 'conv_5', 'bn_5', 'relu_5', 'conv_6', 'bn_6', 'relu_6', 'conv_7', 'bn_7', 'relu_7', 'shortcut_8', 'conv_9', 'bn_9', 'relu_9', 'conv_10', 'bn_10', 'relu_10', 'shortcut_11', 'conv_12', 'bn_12', 'relu_12', 'conv_13', 'bn_13', 'relu_13', 'conv_14', 'bn_14', 'relu_14', 'shortcut_15', 'conv_16', 'bn_16', 'relu_16', 'conv_17', 'bn_17', 'relu_17', 'shortcut_18', 'conv_19', 'bn_19', 'relu_19', 'conv_20', 'bn_20', 'relu_20', 'shortcut_21', 'conv_22', 'bn_22', 'relu_22', 'conv_23', 'bn_23', 'relu_23', 'shortcut_24', 'conv_25', 'bn_25', 'relu_25', 'conv_26', 'bn_26', 'relu_26', 'shortcut_27', 'conv_28', 'bn_28', 'relu_28', 'conv_29', 'bn_29', 'relu_29', 'shortcut_30', 'conv_31', 'bn_31', 'relu_31', 'conv_32', 'bn_32', 'relu_32', 'shortcut_33', 'conv_34', 'bn_34', 'relu_34', 'conv_35', 'bn_35', 'relu_35', 'shortcut_36', 'conv_37', 'bn_37', 'relu_37', 'conv_38', 'bn_38', 'relu_38', 'conv_39', 'bn_39', 'relu_39', 'shortcut_40', 'conv_41', 'bn_41', 'relu_41', 'conv_42', 'bn_42', 'relu_42', 'shortcut_43', 'conv_44', 'bn_44', 'relu_44', 'conv_45', 'bn_45', 'relu_45', 'shortcut_46', 'conv_47', 'bn_47', 'relu_47', 'conv_48', 'bn_48', 'relu_48', 'shortcut_49', 'conv_50', 'bn_50', 'relu_50', 'conv_51', 'bn_51', 'relu_51', 'shortcut_52', 'conv_53', 'bn_53', 'relu_53', 'conv_54', 'bn_54', 'relu_54', 'shortcut_55', 'conv_56', 'bn_56', 'relu_56', 'conv_57', 'bn_57', 'relu_57', 'shortcut_58', 'conv_59', 'bn_59', 'relu_59', 'conv_60', 'bn_60', 'relu_60', 'shortcut_61', 'conv_62', 'bn_62', 'relu_62', 'conv_63', 'bn_63', 'relu_63', 'conv_64', 'bn_64', 'relu_64', 'shortcut_65', 'conv_66', 'bn_66', 'relu_66', 'conv_67', 'bn_67', 'relu_67', 'shortcut_68', 'conv_69', 'bn_69', 'relu_69', 'conv_70', 'bn_70', 'relu_70', 'shortcut_71', 'conv_72', 'bn_72', 'relu_72', 'conv_73', 'bn_73', 'relu_73', 'shortcut_74', 'conv_75', 'bn_75', 'relu_75', 'conv_76', 'bn_76', 'relu_76', 'conv_77', 'bn_77', 'relu_77', 'conv_78', 'bn_78', 'relu_78', 'conv_79', 'bn_79', 'relu_79', 'conv_80', 'bn_80', 'relu_80', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'relu_84', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'relu_87', 'conv_88', 'bn_88', 'relu_88', 'conv_89', 'bn_89', 'relu_89', 'conv_90', 'bn_90', 'relu_90', 'conv_91', 'bn_91', 'relu_91', 'conv_92', 'bn_92', 'relu_92', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'relu_96', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'relu_99', 'conv_100', 'bn_100', 'relu_100', 'conv_101', 'bn_101', 'relu_101', 'conv_102', 'bn_102', 'relu_102', 'conv_103', 'bn_103', 'relu_103', 'conv_104', 'bn_104', 'relu_104', 'conv_105', 'permute_106', 'yolo_106']\n",
            "[[200]\n",
            " [227]\n",
            " [254]]\n",
            "['yolo_82', 'yolo_94', 'yolo_106']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdTvs_qJgmld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8bf90121-23be-4244-8ffb-37ceb3fd9b91"
      },
      "source": [
        "# initialize the video stream and pointer to output video file\n",
        "print(\"[INFO] accessing video stream...\")\n",
        "vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\n",
        "writer = None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] accessing video stream...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJcfAW7jBXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYa0uH1Zvjbz",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2020/05/social_distance_detector_people_detections.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q44HokUh0RP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# read the next frame from the file\n",
        "\t(grabbed, frame) = vs.read()\n",
        "\n",
        "\t# if the frame was not grabbed, then we have reached the end\n",
        "\t# of the stream\n",
        "\tif not grabbed:\n",
        "\t\tbreak\n",
        "\n",
        "\t# resize the frame and then detect people (and only people) in it\n",
        "\tframe = imutils.resize(frame, width=700)\n",
        "\tresults = detect_people(frame, net, ln,\n",
        "\t\tpersonIdx=LABELS.index(\"person\"))\n",
        "\n",
        "\t# initialize the set of indexes that violate the minimum social\n",
        "\t# distance\n",
        "\tviolate = set()\n",
        "\n",
        "\t# ensure there are *at least* two people detections (required in\n",
        "\t# order to compute our pairwise distance maps)\n",
        "\tif len(results) >= 2:\n",
        "\t\t# extract all centroids from the results and compute the\n",
        "\t\t# Euclidean distances between all pairs of the centroids\n",
        "\t\tcentroids = np.array([r[2] for r in results])\n",
        "\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
        "\n",
        "\t\t# loop over the upper triangular of the distance matrix\n",
        "\t\tfor i in range(0, D.shape[0]):\n",
        "\t\t\tfor j in range(i + 1, D.shape[1]):\n",
        "\t\t\t\t# check to see if the distance between any two\n",
        "\t\t\t\t# centroid pairs is less than the configured number\n",
        "\t\t\t\t# of pixels\n",
        "\t\t\t\tif D[i, j] < MIN_DISTANCE:\n",
        "\t\t\t\t\t# update our violation set with the indexes of\n",
        "\t\t\t\t\t# the centroid pairs\n",
        "\t\t\t\t\tviolate.add(i)\n",
        "\t\t\t\t\tviolate.add(j)\n",
        "\n",
        "\t# loop over the results\n",
        "\tfor (i, (prob, bbox, centroid)) in enumerate(results):\n",
        "\t\t# extract the bounding box and centroid coordinates, then\n",
        "\t\t# initialize the color of the annotation\n",
        "\t\t(startX, startY, endX, endY) = bbox\n",
        "\t\t(cX, cY) = centroid\n",
        "\t\tcolor = (0, 255, 0) # green\n",
        "\n",
        "\t\t# if the index pair exists within the violation set, then\n",
        "\t\t# update the color\n",
        "\t\tif i in violate:\n",
        "\t\t\tcolor = (0, 0, 255) # red\n",
        "\n",
        "\t\t# draw (1) a bounding box around the person and (2) the\n",
        "\t\t# centroid coordinates of the person,\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\t\tcv2.circle(frame, (cX, cY), 5, color, 1)\n",
        "\n",
        "\t# draw the total number of social distancing violations on the\n",
        "\t# output frame\n",
        "\ttext = \"Social Distancing Violations: {}\".format(len(violate))\n",
        "\tcv2.putText(frame, text, (10, frame.shape[0] - 25),\n",
        "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
        "\n",
        "\t# check to see if the output frame should be displayed to our\n",
        "\t# screen\n",
        "\tif args[\"display\"] > 0:\n",
        "\t\t# show the output frame\n",
        "\t\tcv2_imshow(frame)\n",
        "\t\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t\t# if the `q` key was pressed, break from the loop\n",
        "\t\tif key == ord(\"q\"):\n",
        "\t\t\tbreak\n",
        "\n",
        "\t# if an output video file path has been supplied and the video\n",
        "\t# writer has not been initialized, do so now\n",
        "\tif args[\"output\"] != \"\" and writer is None:\n",
        "\t\t# initialize our video writer\n",
        "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 25,\n",
        "\t\t\t(frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "\t# if the video writer is not None, write the frame to the output\n",
        "\t# video file\n",
        "\tif writer is not None:\n",
        "\t\twriter.write(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsSlJTxPh9Yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv output.avi 'drive/My Drive/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khz-s1rQ12Sc",
        "colab_type": "text"
      },
      "source": [
        "### Limitations and future improvements\n",
        "\n",
        "1. the first step to improving our social distancing detector is to utilize a proper camera calibration.\n",
        "2. Secondly, you should consider applying a top-down transformation of your viewing angle![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2020/05/social_distance_detector_topdown.jpg)\n",
        "3. My third recommendation is to improve the people detection process.\n",
        "\n",
        " OpenCV’s YOLO implementation is quite slow not because of the model itself but because of the additional post-processing required by the model.\n",
        "\n",
        "To further speedup the pipeline, consider utilizing a Single Shot Detector (SSD) running on your GPU — that will improve frame throughput rate considerably.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}