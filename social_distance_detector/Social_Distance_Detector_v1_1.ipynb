{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Social Distance Detector_v1.1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCIVO6knFUb-"
      },
      "source": [
        "- 튜토리얼 자료 : [Pyimagesearch - OpenCV Social Distancing Detector](https://www.pyimagesearch.com/2020/06/01/opencv-social-distancing-detector/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgusMhBW7Lq_"
      },
      "source": [
        "## 사전 준비 - 데이터 다운로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjagmA1iPfdu"
      },
      "source": [
        "### 어떤 준비가 필요한가\n",
        "- 해당 튜토리얼의 YOLOv3 를 실행시키기 위해 파일이 필요.\n",
        "  - 원 튜토리얼 구조대로 yolo-coco 에 해당 파일을 넣어둠.\n",
        "- 다운로드에 시간이 걸리므로 미리 실행이 필요함\n",
        "- 정상적으로 동작했다면 좌측 파일 네비게이터에서 yolo-coco 폴더와 input 으로 사용할 비디오가 보임\n",
        "- runtime이 종료되면 파일도 사라지므로 여러 번 예제를 사용한다면, 비교적 속도가 빠른 `방법.2`를 추천\n",
        "\n",
        "### 방법 1. 다운로드\n",
        "  - 웹에서 파일 다운로드\n",
        "  - Darknet(dnn framework) repo에서 다운받음   \n",
        "    - [github repo](https://github.com/pjreddie/darknet)\n",
        "    - [YOLO v3 site](https://pjreddie.com/darknet/yolo/)\n",
        "\n",
        "### 방법 2. 사전에 업로드 된 구글드라이브 파일 사용\n",
        "- 자신의 구글 드라이브에 yolo-coco 데이터와, 샘플 영상을 업로드하고 가져와서 사용함. \n",
        "- 각 변수에 데이터가 위치한 자신의 구글 드라이브 path 적어줌. \n",
        "  - `GDRIVE_PATH_YOLO` : yolo-coco 파일들\n",
        "  - `GDRIVE_PATH_INPUT` : 입력으로 사용할 비디오 파일\n",
        "  - 본 예제에서는 편의를 위해 최상위 폴더에 데이터를 저장해둠.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AIMLZhK86K_"
      },
      "source": [
        "### 방법 1. 파일 다운로드 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SJGJ39dOxE8",
        "outputId": "6438fed3-9d1d-4a7d-e3cc-a4dddb496e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# 파일 다운로드\n",
        "# yolo-coco\n",
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights\n",
        "\n",
        "# input 비디오 \n",
        "!wget https://raw.githubusercontent.com/dhrim/pis_study/master/social_distance_detector/pedestrians.mp4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-22 04:33:22--  https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8342 (8.1K) [text/plain]\n",
            "Saving to: ‘yolov3.cfg’\n",
            "\n",
            "\ryolov3.cfg            0%[                    ]       0  --.-KB/s               \ryolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-22 04:33:22 (89.8 MB/s) - ‘yolov3.cfg’ saved [8342/8342]\n",
            "\n",
            "--2020-10-22 04:33:22--  https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 625 [text/plain]\n",
            "Saving to: ‘coco.names’\n",
            "\n",
            "coco.names          100%[===================>]     625  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-22 04:33:22 (41.6 MB/s) - ‘coco.names’ saved [625/625]\n",
            "\n",
            "--2020-10-22 04:33:22--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights        5%[>                   ]  12.21M  60.7KB/s    eta 30m 39s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXrEhGxUElnJ"
      },
      "source": [
        "# yolo-coco 관련 파일은 폴더 생성 후 이동\n",
        "!mkdir yolo-coco\n",
        "!mv yolov3.cfg yolo-coco/\n",
        "!mv coco.names yolo-coco/\n",
        "!mv yolov3.weights yolo-coco/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANM1-QEA9RaQ"
      },
      "source": [
        "### 방법 2. 구글드라이브 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqD5bVrf8Cl2"
      },
      "source": [
        "# 자신의 파일 절대 경로에 맞게 수정\n",
        "GDRIVE_PATH_YOLO = \"/content/drive/My\\ Drive/yolo-coco\"\n",
        "GDRIVE_PATH_INPUT = \"/content/drive/My\\ Drive/pedestrians.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yknz7sqm9ZOG"
      },
      "source": [
        "# google drive mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN4bKOK7VH8y"
      },
      "source": [
        "# 현재 위치(./)로 파일을 가져옴\n",
        "!cp -r $GDRIVE_PATH_YOLO ./ \n",
        "!cp -r $GDRIVE_PATH_INPUT ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlV-NuK00_jT"
      },
      "source": [
        "# yolo-coco 관련 파일은 폴더 생성 후 이동\n",
        "!mkdir yolo-coco\n",
        "!mv yolov3.cfg yolo-coco/\n",
        "!mv coco.names yolo-coco/\n",
        "!mv yolov3.weights yolo-coco/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejqRVrVrZNDv"
      },
      "source": [
        "## 주제 접근 \n",
        "- 비디오 내 사람 간의 거리 측정 - 사회적 거리두기 준수 여부를 알 수 있음\n",
        " ![social distancing](https://www.pyimagesearch.com/wp-content/uploads/2020/05/social_distance_detector_example.png)\n",
        "- Using computer vision technology based on OpenCV and YOLO-based deep learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glvNqK9qVfBf"
      },
      "source": [
        "## 관련 개념\n",
        "### OpenCV ( Open Source Computer Vision)\n",
        "- 실시간 이미지 프로세싱에 중점을 둔 라이브러리. 인텔이 처음 개발.\n",
        "- 기초 참고\n",
        "  - [OpenCV-Python Tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/)\n",
        "  - [위 자료 바탕 한글 스터디 진행 자료](https://opencv-python.readthedocs.io/en/latest/)\n",
        "    - [zzsza - OpenCV - 이미지/비디오 읽기](https://zzsza.github.io/data/2018/01/23/opencv-1/)\n",
        "\n",
        "### CNN\n",
        "- 뉴럴넷이 통계 모델을 구축하는 것을 스스로 학습함. \n",
        "- 엄청 많은 이미지에서 패턴을 학습함. 이미지/영상을 입력하면 올바른 레이블(이름표)과 연결하는 것.\n",
        "\n",
        "### object detection\n",
        "- modern object(image) classifier 수천 개 object를 분류해낼 수 있음. 여기서 더 나아가, object의 what, where 등의 추가 정보까지 파악이 가능함. \n",
        "- 한 이미지 안에 있는 모든 사물을 찾아내서 테두리 박스(bounding box, bbox) 를 치고 그것이 무엇인지 알아내는 것\n",
        "- 분류뿐만 아니라 사이즈, 위치 등의 다른 context 정보까지 파악할 수 있게 됨\n",
        "- real-time 을 위해서는 detection 속도가 중요함. \n",
        "  - 예를 들어, 자율주행 자동차를 상상해보면, 장애물을 느리게 인식하는 것은 매우 치명적이 될 수 있음 \n",
        "\n",
        "→ 초당 20 이미지 인식가능한 속도(1/500초) YOLO가 2016년 나옴!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoUnzLhFF2-J"
      },
      "source": [
        "### YOLO (You Only Look Once)\n",
        "- [YOLOv3 사이트](https://pjreddie.com/darknet/yolo/) / [github repo YOLOv3](https://github.com/pjreddie/darknet)\n",
        "  - 속도가 빠르고 제한없는 opensource! [https://github.com/pjreddie/darknet/blob/master/LICENSE](https://github.com/pjreddie/darknet/blob/master/LICENSE)\n",
        "![image](https://user-images.githubusercontent.com/17819874/95456281-11a44280-09aa-11eb-93c4-dcfb85537b96.png)\n",
        "\n",
        "- 이전과 뭐가 달라졌지?\n",
        "  - sliding window, DPM, R-CNN 모두 detection 에  region-based classifiers 를 사용\n",
        "  - 많은 횟수의 detection 을 필요로 하므로 많은 evaluation 이 필요함\n",
        "  - 그러면 이렇게 해서 한 번에 detection 하게 하자\n",
        "    1. resize image\n",
        "    2. run CNN\n",
        "    3. Threshold detection\n",
        "\n",
        "- YOLO 개발자가 직접 설명하는 내용\n",
        "  - [Youtube - You Only Look Once: Unified, Real-Time Object Detection - ComputerVisionFoundation Videos | 4:48](https://youtu.be/NM6lrxy0bxs?t=288)\n",
        "  - image 를 grid를 그려 각 cell 로 나눔\n",
        "  - 각 cell을 중심으로 물체의 box(경계)를 찾고 confidence를 예측함.  (\"Each cell predicts boxes and confidence: P(Object)\")\n",
        "  - 해당 cell 이 confidence 가 낮아도 상관없음(예. 바닥의 한 부분) 그건 나중에 걸러내면 되니까.\n",
        "  - A. 신뢰도에 따라서 순위를 매긴 object map 이 있음. 지금은 object 어디있는지 신경쓰지마\n",
        "    ![ppt01-Youtube - You Only Look Once: Unified, Real-Time Object Detection - ComputerVisionFoundation Videos](https://user-images.githubusercontent.com/17819874/96823961-a3ab5100-1468-11eb-8c67-e84c3dc18a0a.png)\n",
        "  - B. 각 셀은 확률적으로 분류됨  conditioned on object : P(Car | Object)\n",
        "    ![ppt02-Youtube - You Only Look Once: Unified, Real-Time Object Detection - ComputerVisionFoundation Videos](https://user-images.githubusercontent.com/17819874/96824316-7e6b1280-1469-11eb-9450-116397ddac80.png)\n",
        "  - A 랑 B를 combine\n",
        "  - 중복되지 않고 confidence 가 높은 것만 추림 - NMS(non-maxima suppression)\n",
        "- 함께 보면 좋은 자료\n",
        "  - YOLO 개발자 Joseph Redmon 가 YOLO 소개하는 영상\n",
        "    - [CVPR 2016 - You Only Look Once: Unified, Real-Time Object Detection](https://youtu.be/NM6lrxy0bxs)\n",
        "    - [TED - how_computers_learn_to_recognize_objects_instantly - Joseph Redmon](https://www.ted.com/talks/joseph_redmon_how_computers_learn_to_recognize_objects_instantly?language=ko)\n",
        "  - [YOLO 설명 by deepsystem.io](https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000)\n",
        "\n",
        "\n",
        "- 참고. 현재(2020.10.08) YOLOv3 이후 아래 repo는 업데이트 되지 않음\n",
        "  - YOLO 원 개발자 Joseph Redmon는 social impact를 고려해 더 이상 YOLO 개발을 하지 않겠다고 선언함. [twitter link](https://twitter.com/pjreddie/status/1230523827446091776) \n",
        "  - 이후 다른 creater가 YOLOv4, YOLOv5 를 만들어 나옴. \n",
        "    - [YOLOv4 정보 in dartnet repo](https://github.com/pjreddie/darknet#yolov4)\n",
        "    - YOLOv4 vs YOLOv5 와 얽힌 이야기는 이 article  참고 - [Responding to the Controversy about YOLOv5 by Joseph Nelson, Jacob Solawetz, roboflow](https://blog.roboflow.com/yolov4-versus-yolov5/)\n",
        "     - [YOLOv5 is Here!\n",
        " by Mihir Rajput, towards datascience](https://towardsdatascience.com/yolo-v5-is-here-b668ce2a4908) / [repo](https://github.com/ultralytics/yolov5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Rk5p9HjcAk"
      },
      "source": [
        "### non-maxima suppression\n",
        "- 여러 경계 상자가 검출되었을 경우, 남길 하나의 경계상자를 찾는 것\n",
        "![](https://www.pyimagesearch.com/wp-content/uploads/2014/10/nms_slow_01.jpg)\n",
        "- 좀 더 자세히는\n",
        "- 인용\u001d: [dyndy - 블로그 NMS(non-maximum-suppression)](https://dyndy.tistory.com/275)\n",
        "> \"각종 boundingbox + 각 box에 object가 있을 확률 (class별 확률)들이 나오게 되는데, 이중 겹치는 부분 (차 한대에 여러가지 boundingbox가 그려지는 경우와 같은)을 제거하기 위한 방법으로 사용된다.\"\n",
        "  1. 동일한 클래스에 대해 높은-낮은 confidence 순서로 정렬한다. (line 13)\n",
        "  2. 가장 confidence가 높은 boundingbox와 IOU가 일정 이상인 boundingbox는 동일한 물체를 detect했다고 판단하여 지운다.(16~20) 보통 50%(0.5)이상인 경우 지우는 경우를 종종 보았다.  \n",
        "\n",
        "  ![IOU](https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png)\n",
        "\n",
        "-> 현재 실습에서는 config 에서 0.3으로 설정 `NMS_THRESH = 0.3` (the threshold when applying non-maxima suppression)\n",
        "\n",
        "- 더 알고 싶다면, \n",
        "  - [towardsdatascience - Non-maximum Suppression (NMS)](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c)\n",
        "  - [deepsystem.io YOLO 설명 39page~ ](https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.g137784ab86_4_4327)\n",
        "  - [pyimagesearch - Non-Maximum Suppression for Object Detection in Python](https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlnsI-K_aB3J"
      },
      "source": [
        "## Using OpenCV, computer vision, and deep learning for social distancing\n",
        "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2020/05/social_distance_detector_steps.png)\n",
        "...\n",
        "- We can use OpenCV, computer vision, and deep learning to implement social distancing detectors.\n",
        "\n",
        "- The steps to build a social distancing detector include:\n",
        "\n",
        "1. 비디오 안 모든 '사람'들에게 object detection 적용 \n",
        "- [people counter tutorial](https://www.pyimagesearch.com/2018/08/13/opencv-people-counter/) 참고\n",
        "\n",
        "2. 감지된 사람들(detected people)사이 거리를 계산\n",
        "\n",
        "3. '2'의 거리를 기반으로 두 사람이 몇 픽셀 떨어져있는지 구함\n",
        "\n",
        "  - 픽셀을 실제 거리로 바꾸려면 카메라 보정 등이 필요\n",
        "  - 위 방법 보다 쉬운 triangle similarity 보정을 할 수 있음 (참고. [Find distance from camera to object/marker using Python and OpenCV](https://www.pyimagesearch.com/2015/01/19/find-distance-camera-objectmarker-using-python-opencv/))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nDr1XOGg4Do"
      },
      "source": [
        "## Project structure\n",
        "- 본 파일에서는 아래 프로젝트 구조를 실습 편의를 위해 하나의 파일로 만들어둠.\n",
        "- pyimagesearch 튜토리얼 구조 \n",
        "```\n",
        "OpenCV Social Distancing Detector\n",
        ".\n",
        "├── pyimagesearch\n",
        "│   ├── __init__.py\n",
        "│   ├── detection.py\n",
        "│   └── social_distancing_config.py\n",
        "├── yolo-coco\n",
        "│   ├── coco.names\n",
        "│   ├── yolov3.cfg\n",
        "│   └── yolov3.weights\n",
        "├── output.avi\n",
        "├── pedestrians.mp4\n",
        "└── social_distance_detector.py\n",
        "2 directories, 9 files\n",
        "```\n",
        "\n",
        "Our YOLO object detector files including the CNN architecture definition, pre-trained weights, and class names are housed in the yolo-coco/ directory. **This YOLO model is compatible with OpenCV’s DNN module.**\n",
        "\n",
        "- yolo-coco \n",
        "  - CNN architecture definition, pre-trained weights 등 정보가 있음. 이 실습에서는 사전 준비 셀에서 미리 다운로드 해둠. \n",
        "  - `yolov3.cfg` : Configuration file with Layers (input data 차원, layer 갯수 등)\n",
        "  - `yolov3.weights` : pre-trained weights, ready to use for Detection.\n",
        "  - `coco.names` : 80 names of objects (labels) that can be Detected on the image.\n",
        "- input : pedestrians.mp4 / CCTV, 카메라 고정, 행인들이 카메라 앞을 지나감\n",
        "- output : output.mp4\n",
        "  1. object detection 행인 감지 - box 형태로 boundary 보여줌\n",
        "  2. 위반 표시 \n",
        "  - social distancing violations: 위반한 object 수, object 간 pixel 계산\n",
        "    - 두 object 쌍과의 pixel 거리 계산해서 설정값 pixel 이하는 위반\n",
        "    - 빨간색 박스로 표시 / 정상은 초록색 box \n",
        "    - default 설정 : `MIN_DISTANCE` 50px (`social_distancing_config.py`)\n",
        "\n",
        "- `/pyimagesearch`\n",
        "  - `detection.py` : object detection module\n",
        "  - `social_distancing_config.py` : config 파일\n",
        "-  `social_distance_detector.py` \n",
        "  - 주 script\n",
        "  - input은 video files and webcam streams 둘 다 지원\n",
        "  - looping over frames of a video stream, 계속 object detection 유지\n",
        "  - 거리두기 위반 검출 : object 간 pixel 거리 감지\n",
        "  - output 비디오 생성\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgWJhEp_Sfl-"
      },
      "source": [
        "\n",
        "## * social_distancing_config.py: A Python file holding a number of constants in one convenient place.\n",
        "(from pyimagesearch import social_distancing_config as config)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y2JlCKcScO7"
      },
      "source": [
        "# boolean indicating if NVIDIA CUDA GPU should be used \n",
        "#(requires that OpenCV’s “dnn” module be installed with NVIDIA GPU support).\n",
        "USE_GPU = False\n",
        "\n",
        "# base path to YOLO directory\n",
        "MODEL_PATH = \"yolo-coco\"\n",
        "\n",
        "# initialize minimum probability to filter weak detections along with\n",
        "# the threshold when applying non-maxima suppression\n",
        "MIN_CONF = 0.3\n",
        "NMS_THRESH = 0.3\n",
        "\n",
        "# define the minimum safe distance (in pixels) that two people can be\n",
        "# from each other\n",
        "MIN_DISTANCE = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyAdXiB7Sn4X"
      },
      "source": [
        "### * detection.py :  YOLO object detection with OpenCV \n",
        "( from pyimagesearch.detection import detect_people)\n",
        "- Our detect_people utility function, which detects people in video streams using the YOLO object detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ2wTqsxkpuX"
      },
      "source": [
        "*detect_people*; the function accepts four parameters:\n",
        "\n",
        "### Parameter\n",
        "- frame: The frame from your video file or directly from your webcam\n",
        "  - 여기서는 720 으로 resize 해서 사용\n",
        "- net: The pre-initialized and **pre-trained** YOLO object detection model\n",
        "- ln: The YOLO CNN **output layer** names\n",
        "- personIdx: The YOLO model can detect many types of objects; this index is **specifically for the person** class, as we won’t be considering other objects\n",
        "\n",
        "### Return result\n",
        "We then initialize our results list, which the function ultimately returns.\n",
        "- Result   \n",
        "  (1) the person prediction probability  \n",
        "  (2) bounding box coordinates for the detection  \n",
        "  (3) the centroid of the object.    \n",
        "\n",
        "<img width=\"98\" alt=\"output avi 2020-10-08 17-45-35\" src=\"https://user-images.githubusercontent.com/17819874/95435958-35598f80-098e-11eb-8c82-4a4672b86781.png\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6p8e_XbSmHt"
      },
      "source": [
        "# import the necessary packages\n",
        "# from .social_distancing_config import NMS_THRESH\n",
        "# from .social_distancing_config import MIN_CONF\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# frame : 탐지 대상 영상\n",
        "# net : YOLO 모델\n",
        "# in : YOLO 모델의 출력 레이어 이름들. ['yolo_82', 'yolo_94', 'yolo_106']\n",
        "# personIdx : 카테고리 person의 index.\n",
        "def detect_people(frame, net, ln, personIdx=0):\n",
        "\t# grab the dimensions of the frame and  initialize the list of results\n",
        "\n",
        "  # image.shape 함수는 이미지 파일의 모양을 return합니다.\n",
        "\t# 순서대로 Y축, X축, 채널의 수 이미지 읽기 (Image Read) https://zzsza.github.io/data/2018/01/23/opencv-1/\n",
        "\t(H, W) = frame.shape[:2] \n",
        "\tresults = []\n",
        "\n",
        "\t# construct a blob from the input frame and then perform a forward\n",
        "\t# pass of the YOLO object detector, giving us our bounding boxes\n",
        "\t# and associated probabilities\n",
        "\n",
        "\t# 받은 frame(이미지) 전처리 https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/\n",
        "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
        "\t\tswapRB=True, crop=False)\n",
        "  # YOLO 수행\n",
        "\tnet.setInput(blob)\n",
        "\tlayerOutputs = net.forward(ln)\n",
        "\n",
        "\t# initialize our lists of detected bounding boxes, centroids, and\n",
        "\t# confidences, respectively\n",
        "\tboxes = []\n",
        "\tcentroids = []\n",
        "\tconfidences = []\n",
        "\n",
        "\t# loop over each of the layer outputs\n",
        "\tfor output in layerOutputs:\n",
        "\t\t# loop over each of the detections\n",
        "\t\tfor detection in output:\n",
        "\t\t\t# extract the class ID and confidence (i.e., probability)\n",
        "\t\t\t# of the current object detection\n",
        "\t\t\tscores = detection[5:]\n",
        "\t\t\tclassID = np.argmax(scores)\n",
        "\t\t\tconfidence = scores[classID]\n",
        "\n",
        "\t\t\t# filter detections by \n",
        "\t\t\t# (1) 감지된 게 인간이고 \n",
        "\t\t\t# ensuring that the object detected was a person\n",
        "\t\t\t# (2) 최소 confidence(MIN_CONF = 0.3) 를 충족시키면\n",
        "\t\t\t# that the minimum confidence is met\n",
        "\t\t\tif classID == personIdx and confidence > MIN_CONF:\n",
        "\t\t\n",
        "\t\t\t\t# YOLO 에서는 object의 중앙 좌표만 나오므로 가로,세로 사용해서 박스 모양으로 만들어줌\n",
        "\t\t\t\t# scale the bounding box coordinates back \n",
        "\t\t\t\t# relative to the size of the image, \n",
        "\t\t\t\t# keeping in mind that YOLO actually returns the center (x, y)-coordinates of the bounding box \n",
        "\t\t\t\t# followed by the boxes' width and height\n",
        "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
        "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "\t\t\t\t# use the center (x, y)-coordinates to derive the top\n",
        "\t\t\t\t# and and left corner of the bounding box\n",
        "\t\t\t\tx = int(centerX - (width / 2))\n",
        "\t\t\t\ty = int(centerY - (height / 2))\n",
        "\t\t\n",
        "\t\t\t\t# 박스 좌표, centroids, confidences 값 저장\n",
        "\t\t\t\t# update our list of bounding box coordinates,\n",
        "\t\t\t\t# centroids, and confidences\n",
        "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
        "\t\t\t\tcentroids.append((centerX, centerY))\n",
        "\t\t\t\tconfidences.append(float(confidence))\n",
        "\n",
        "\t# 겹치는 bounding boxes 삭제 - NMS 사용해서\n",
        "\t# apply non-maxima suppression to suppress weak, overlapping\n",
        "\t# bounding boxes\n",
        "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
        "\n",
        "\t# result 형태 만들기 - 각각의 값들을 대한 결과 형태로 만들어줌\n",
        "\t# ensure at least one detection exists\n",
        "\tif len(idxs) > 0:\n",
        "\t\t# loop over the indexes we are keeping\n",
        "\t\tfor i in idxs.flatten():\n",
        "\t\t\t# extract the bounding box coordinates\n",
        "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
        "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "\t\t\t# update our results list to consist of the person\n",
        "\t\t\t# prediction probability, bounding box coordinates,\n",
        "\t\t\t# and the centroid\n",
        "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
        "\t\t\tresults.append(r)\n",
        "\n",
        "\t# return the list of results\n",
        "\treturn results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6yHlNDcTEGv"
      },
      "source": [
        "### Social Distance Detector\n",
        "(social_distance_detector.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZobH5kGQVyZ",
        "cellView": "code"
      },
      "source": [
        "# import the necessary packages\n",
        "# from pyimagesearch import social_distancing_config as config\n",
        "# from pyimagesearch.detection import detect_people\n",
        "from scipy.spatial import distance as dist\n",
        "import numpy as np\n",
        "# import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25WnH58dheN2"
      },
      "source": [
        "# USAGE 사용 방법\n",
        "# python social_distance_detector.py --input pedestrians.mp4\n",
        "# python social_distance_detector.py --input pedestrians.mp4 --output output.avi\n",
        "# python social_distance_detector.py --input pedestrians.mp4 --output output.avi  --display 0\n",
        "# --display: 기본적으로 각 프레임을 처리 할 때 소셜 거리 애플리케이션을 화면에 표시합니다.  0: 백그라운드에서 스트림을 처리합니다.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddFBZmQvhgOE"
      },
      "source": [
        "### USAGE 사용 방법\n",
        "```\n",
        "python social_distance_detector.py --input pedestrians.mp4\n",
        "python social_distance_detector.py --input pedestrians.mp4 --output output.avi\n",
        "python social_distance_detector.py --input pedestrians.mp4 --output output.avi  --display 0\n",
        "```\n",
        "\n",
        "- `--display` : 기본적으로 각 프레임을 처리 할 때 소셜 거리 애플리케이션을 화면에 표시합니다.  0: 백그라운드에서 스트림을 처리합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ej78L_g5nY"
      },
      "source": [
        "# construct the argument parse and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\n",
        "# \thelp=\"path to (optional) input video file\")\n",
        "# ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\n",
        "# \thelp=\"path to (optional) output video file\")\n",
        "# ap.add_argument(\"-d\", \"--display\", type=int, default=1,\n",
        "# \thelp=\"whether or not output frame should be displayed\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "args = {'input':'pedestrians.mp4','output':'output.avi','display':0}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umm3EbzOhRgN"
      },
      "source": [
        "### 초기 설정\n",
        "- loading coco 레이블 \n",
        "- YOLO 경로 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN5hxpCEBpa_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfTNxPfnb2l9"
      },
      "source": [
        "# # load the COCO class labels our YOLO model was trained on\n",
        "# labelsPath = os.path.sep.join([config.MODEL_PATH, \"coco.names\"])\n",
        "# LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
        "labelsPath = os.path.sep.join([MODEL_PATH, \"coco.names\"])\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPiiHj0viu5J"
      },
      "source": [
        "# print(LABELS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8BkQEewdW8X"
      },
      "source": [
        "# derive the paths to the YOLO weights and model configuration\n",
        "weightsPath = os.path.sep.join([MODEL_PATH, \"yolov3.weights\"])\n",
        "configPath = os.path.sep.join([MODEL_PATH, \"yolov3.cfg\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWjjytBOkCot"
      },
      "source": [
        "### Using OpenCV’s DNN module, we load our YOLO net into memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V8oLrOXdkbR"
      },
      "source": [
        "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "# OpenCV의 DNN 모듈을 사용하여 YOLO를 로드\n",
        "print(\"[INFO] loading YOLO from disk...\")\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QytcFBlrUMRK"
      },
      "source": [
        "print(net.getUnconnectedOutLayersNames())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QldJsiqIiQDN"
      },
      "source": [
        "### GPU 사용 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAX4_HI5iORt"
      },
      "source": [
        "If you have the USE_GPU option set in the config, then the backend processor is set to be your NVIDIA CUDA-capable GPU. If you don’t have a CUDA-capable GPU, ensure that the configuration option is set to False so that your CPU is the processor used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQOiTQThdpD2"
      },
      "source": [
        "# check if we are going to use GPU\n",
        "#if USE_GPU == True:\n",
        "\t# set CUDA as the preferable backend and target\n",
        "#print(\"[INFO] setting preferable backend and target to CUDA...\")\n",
        "#net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
        "#net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bISeCKcciYjl"
      },
      "source": [
        "### 결과 출력을 위해 YOLO에서 출력 레이어 이름을 가져옴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNnxHlw5e0e8"
      },
      "source": [
        "# determine only the *output* layer names that we need from YOLO\n",
        "ln = net.getLayerNames()\n",
        "print(\"ln :\", ln)\n",
        "# ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "ln = net.getUnconnectedOutLayersNames()\n",
        "print(net.getUnconnectedOutLayers())\n",
        "print(ln)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbw2FgRvixFA"
      },
      "source": [
        "### input 비디오 스트림 접근"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdTvs_qJgmld"
      },
      "source": [
        "# initialize the video stream and pointer to output video file\n",
        "print(\"[INFO] accessing video stream...\")\n",
        "# vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\n",
        "vs = cv2.VideoCapture(\"pedestrians.mp4\")\n",
        "writer = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q44HokUh0RP"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# input 처리 - 비디오 끝까지 실행\n",
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# read the next frame from the file\n",
        "\t(grabbed, frame) = vs.read()\n",
        "\n",
        "\t# if the frame was not grabbed, then we have reached the end\n",
        "\t# of the stream\n",
        "\tif not grabbed:\n",
        "\t\tbreak\n",
        "\n",
        "\t# 프레임 크기 조정 후 프레임 안 사람들만 감지 - 프레임 조정 안하면 입력 비디오 크기 너무 큼\n",
        "\t# resize the frame and then detect people (and only people) in it\n",
        "\tframe = imutils.resize(frame, width=700)\n",
        "  # 라벨이 person 인 것 detection\n",
        "\tresults = detect_people(frame, net, ln,\n",
        "\t\tpersonIdx=LABELS.index(\"person\"))\n",
        "\n",
        "\t# initialize the set of indexes that violate the minimum social distance\n",
        "\t# 위반자 담을 set \n",
        "\tviolate = set()\n",
        "\n",
        "\t# 프레임 안에 적어도 두 사람이 감지되면, \n",
        "\t# 유클리드 거리 계산으로 두 사람간 거리가 위반 거리 이내이면 violate 에 담음\n",
        "\t# ensure there are *at least* two people detections (required in\n",
        "\t# order to compute our pairwise distance maps)\n",
        "\tif len(results) >= 2:\n",
        "\t\t# extract all centroids from the results and compute the\n",
        "\t\t# Euclidean distances between all pairs of the centroids\n",
        "\t\tcentroids = np.array([r[2] for r in results])\n",
        "\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
        "\n",
        "\t\t# loop over the upper triangular of the distance matrix\n",
        "\t\tfor i in range(0, D.shape[0]):\n",
        "\t\t\tfor j in range(i + 1, D.shape[1]):\n",
        "\t\t\t\t# check to see if the distance between any two\n",
        "\t\t\t\t# centroid pairs is less than the configured number\n",
        "\t\t\t\t# of pixels\n",
        "\t\t\t\tif D[i, j] < MIN_DISTANCE:\n",
        "\t\t\t\t\t# update our violation set with the indexes of\n",
        "\t\t\t\t\t# the centroid pairs\n",
        "\t\t\t\t\tviolate.add(i)\n",
        "\t\t\t\t\tviolate.add(j)\n",
        "\n",
        "\t# 결과 시각화\n",
        "\t# loop over the results\n",
        "\tfor (i, (prob, bbox, centroid)) in enumerate(results):\n",
        "\t\t# 좌표 뽑고, 색은 green으로 설정\n",
        "\t\t# extract the bounding box and centroid coordinates, then\n",
        "\t\t# initialize the color of the annotation\n",
        "\t\t(startX, startY, endX, endY) = bbox\n",
        "\t\t(cX, cY) = centroid\n",
        "\t\tcolor = (0, 255, 0) # green\n",
        "\n",
        "\t\t# 위반 set안에 있으면 빨간색 으로 color 업데이트\n",
        "\t\t# if the index pair exists within the violation set, then\n",
        "\t\t# update the color\n",
        "\t\tif i in violate:\n",
        "\t\t\tcolor = (0, 0, 255) # red\n",
        "\n",
        "\t\t# 그림 그리기\n",
        "\t\t# bounding box(사람 박스)는 네모, 사람 중심 좌표에 동그라미 표시\n",
        "\t\t# (1) a bounding box around the person and \n",
        "\t\t# (2) the centroid coordinates of the person,\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\t\tcv2.circle(frame, (cX, cY), 5, color, 1)\n",
        "\n",
        "\t# 총 위반 건수 그리기 \"Social Distancing Violations: n\"\n",
        "\t# draw the total number of social distancing violations on the\n",
        "\t# output frame\n",
        "\ttext = \"Social Distancing Violations: {}\".format(len(violate))\n",
        "\tcv2.putText(frame, text, (10, frame.shape[0] - 25),\n",
        "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
        "\n",
        "\t# display 인자가 0 이상이면(0이 아니면), 출력 프레임 표시\n",
        "\t# q 키 누르면 루프 종료\n",
        "\t# check to see if the output frame should be displayed to our screen\n",
        "\tif args[\"display\"] > 0:\n",
        "\t\t# show the output frame\n",
        "\t\tcv2_imshow(frame)\n",
        "\t\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t\t# if the `q` key was pressed, break from the loop\n",
        "\t\tif key == ord(\"q\"):\n",
        "\t\t\tbreak\n",
        "\n",
        "\t# 비디오 쓰기 위해 writer 초기화 확인\n",
        "\t# if an output video file path has been supplied and the video\n",
        "\t# writer has not been initialized, do so now\n",
        "\tif args[\"output\"] != \"\" and writer is None:\n",
        "\t\t# initialize our video writer\n",
        "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 25,\n",
        "\t\t\t(frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "\t# output - 비디오 파일로 출력\n",
        "\t# if the video writer is not None, write the frame to the output\n",
        "\t# video file\n",
        "\tif writer is not None:\n",
        "\t\twriter.write(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYa0uH1Zvjbz"
      },
      "source": [
        "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2020/05/social_distance_detector_people_detections.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AixqoEwgllh4"
      },
      "source": [
        "## 결과값 구글 드라이브 최상위 폴더로 옮김"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsSlJTxPh9Yg"
      },
      "source": [
        "!mv output.avi 'drive/My Drive/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khz-s1rQ12Sc"
      },
      "source": [
        "### Limitations and future improvements\n",
        "- 개선되면 좋을 것 : 카메라 보정, 보기 각도 변환, object detection 개선\n",
        "\n",
        "1. the first step to improving our social distancing detector is to utilize a proper camera calibration.\n",
        "2. Secondly, you should consider applying a top-down transformation of your viewing angle![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2020/05/social_distance_detector_topdown.jpg)\n",
        "3. My third recommendation is to improve the people detection process.\n",
        "\n",
        " OpenCV’s YOLO implementation is quite slow not because of the model itself but because of the additional post-processing required by the model.\n",
        "\n",
        "To further speedup the pipeline, consider utilizing a Single Shot Detector (SSD) running on your GPU — that will improve frame throughput rate considerably.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}